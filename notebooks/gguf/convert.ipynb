{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.2.15: Fast Qwen2 patching. Transformers: 4.46.3.\n",
      "   \\\\   /|    GPU: Quadro M4000. Max memory: 7.924 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.0+cu124. CUDA: 5.2. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28.post2. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.2.15 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Paris, the Eiffel Tower stands as a symbol of both engineering prowess and French cultural heritage. Commissioned in 1889 for the Exposition Universelle (World's Fair) to celebrate the centennial of Napoleon I, Gustave Eiffel designed the iron structure to hold up the impressive wrought-iron lattice that spans more than two thousand feet into the sky.\n",
      "\n",
      "The base is approximately sixty-eight meters wide and thirty-two wide at its tallest point, with a total height of nearly five hundred twenty-five feet from ground level or seventy-six meters above its foundation. The towers are located along the banks of the Seine\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"/home/paperspace/naima-lab/model/model\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "    max_seq_length = 2028,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Describe a tall tower in the capital of France.\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 128,\n",
    "                   use_cache = True, temperature = 1.5, min_p = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make: Entering directory '/home/paperspace/naima-lab/notebooks/gguf/llama.cpp'\n",
      "make: Leaving directory '/home/paperspace/naima-lab/notebooks/gguf/llama.cpp'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Makefile:2: *** The Makefile build is deprecated. Use the CMake build instead. For more details, see https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md.  Stop.\n",
      "sh: 1: cmake: not found\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "*** Unsloth: Failed compiling llama.cpp using os.system(...) with error 32512. Please report this ASAP!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_pretrained_gguf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/paperspace/naima-lab/model/model_gguf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/naima-lab/.venv/lib/python3.10/site-packages/unsloth/save.py:1697\u001b[0m, in \u001b[0;36munsloth_save_pretrained_gguf\u001b[0;34m(self, save_directory, tokenizer, quantization_method, first_conversion, push_to_hub, token, private, is_main_process, state_dict, save_function, max_shard_size, safe_serialization, variant, save_peft_format, tags, temporary_location, maximum_memory_usage)\u001b[0m\n\u001b[1;32m   1695\u001b[0m python_install \u001b[38;5;241m=\u001b[39m install_python_non_blocking([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgguf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprotobuf\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   1696\u001b[0m git_clone\u001b[38;5;241m.\u001b[39mwait()\n\u001b[0;32m-> 1697\u001b[0m makefile \u001b[38;5;241m=\u001b[39m \u001b[43minstall_llama_cpp_make_non_blocking\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1698\u001b[0m new_save_directory, old_username \u001b[38;5;241m=\u001b[39m unsloth_save_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39marguments)\n\u001b[1;32m   1699\u001b[0m python_install\u001b[38;5;241m.\u001b[39mwait()\n",
      "File \u001b[0;32m~/naima-lab/.venv/lib/python3.10/site-packages/unsloth/save.py:783\u001b[0m, in \u001b[0;36minstall_llama_cpp_make_non_blocking\u001b[0;34m()\u001b[0m\n\u001b[1;32m    781\u001b[0m check \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcmake llama.cpp -B llama.cpp/build -DBUILD_SHARED_LIBS=OFF -DGGML_CUDA=OFF -DLLAMA_CURL=ON\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 783\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*** Unsloth: Failed compiling llama.cpp using os.system(...) with error \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheck\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Please report this ASAP!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    784\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    785\u001b[0m \u001b[38;5;66;03m# f\"cmake --build llama.cpp/build --config Release -j{psutil.cpu_count()*2} --clean-first --target {' '.join(LLAMA_CPP_TARGETS)}\",\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: *** Unsloth: Failed compiling llama.cpp using os.system(...) with error 32512. Please report this ASAP!"
     ]
    }
   ],
   "source": [
    "model.save_pretrained_gguf(\"/home/paperspace/naima-lab/model/model_gguf\", tokenizer,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
